"use strict";(self.webpackChunkwaterdip=self.webpackChunkwaterdip||[]).push([[75],{1229:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"mlobs","metadata":{"permalink":"/website/blog/mlobs","source":"@site/blog/2023-02-01-what-is-ml-observability/index.md","title":"What is ML Observability?","description":"ML observability is the ability to measure and report on the performance of machine learning models in real time.","date":"2023-02-01T00:00:00.000Z","formattedDate":"February 1, 2023","tags":[{"label":"ML Observability","permalink":"/website/blog/tags/ml-observability"}],"readingTime":2.93,"hasTruncateMarker":false,"authors":[{"name":"Subhankar Biswas","title":"CO-FOUNDER, CEO","url":"https://www.linkedin.com/in/wsubhankarb/","imageURL":"https://github.com/subhankarb.png","key":"subhankar"}],"frontMatter":{"slug":"mlobs","title":"What is ML Observability?","authors":["subhankar"],"tags":["ML Observability"]}},"content":"ML observability is the ability to measure and report on the performance of machine learning models in real time.\\r\\nIt enables organizations to improve model accuracy and reliability by measuring service quality continuously across pre-production and production phases of model life cycles.\\r\\n\\r\\n\\r\\n## **What could possibly go wrong?**\\r\\n\\r\\n### **Data Distribution**\\r\\n\\r\\n  In machine learning, the most basic assumption is that the distribution of data your model is exposed to changes over time.\\r\\n  For example, if you are working on an image recognition task and have been collecting images of cats for a few months,\\r\\n  and then suddenly start collecting images of dogs, it would be unwise to keep using the same model architecture. A model that has been trained on cats may not work very well on dogs\\r\\n  \u2013 even if there are some visual similarities between cats and dogs (big/pointy ears, tails, etc.).\\r\\n\\r\\n| ![ML Data Distribution](./data_distribution.png) |\\r\\n|:--:|\\r\\n|*Training Serving Data Distribution*|\\r\\n\\r\\n\\r\\n### **Training Serving Skew**\\r\\nA common use case for observability is when your model is working well in training but poorly in production.\\r\\nData scientists often find that the data their model was trained on is statistically different from the data they\\r\\nsee in production. This discrepancy can be due to any number of factors: the sample size of your training set\\r\\nmay not be enough to capture all possible conditions and edge cases; some of your features may be correlated;\\r\\nor there may be seasonal or event-driven variations in the data that weren\'t captured by your dataset.\\r\\n\\r\\n\\r\\n## **How to ensure that ML model is working correctly?**\\r\\n\\r\\n| ![ML Monitoring Lifecycle](./ml_monitoring_lifecycle.png) |\\r\\n|:--:|\\r\\n|*ML Monitoring Lifecycle*|\\r\\n\\r\\nML observability has to track the lifecycle of an ML model from its inception through training, validation and deployment.\\r\\nIt encompasses a broad set of capabilities, including the ability to:\\r\\n\\r\\n### Test & Pre-Production Validation\\r\\nIn order to ensure that model behavior conforms to your expectations, you need to monitor the model\u2019s performance during Pre-Production validation.\\r\\nML observability tools allow you to track a model\u2019s performance for each defined slice in the training data, so you can see how well it will generalize when deployed into production.\\r\\n\\r\\n### Monitor Production System\\r\\nWhen model is deployed to production, ML Observability keeps track of all of the input features and output predictions to provide proactive alerts.\\r\\nThese alerts can be used as early warning signs of potential issues with the model.\\r\\nThe user can also use these alerts to debug the model by checking whether any of these inputs have changed since they last checked,\\r\\nor if any of these outputs are not being predicted correctly.\\r\\n\\r\\n### Root Cause Analysis\\r\\nWhen a model in production is failing to perform as expected, the first step toward a resolution is to understand what happened.\\r\\nThis determination can be difficult because the model may have been trained and tested on different data than what it\'s operating on now,\\r\\nor it may have been trained with different hyperparameters than those being used now.\\r\\nIn both cases, the network weights could have changed substantially from their training parameters, meaning that a new best-fit line wouldn\'t exist.\\r\\n\\r\\nWith the help of Observability platform to monitor your models in production, you\'ll be able to determine exactly which distributions in input data,\\r\\nfeatures, ground truth/proxy metrics have contributed to a change in the model\u2019s performance by combining your\\r\\nhistorical data with your model\'s current performance. The result of this analysis will let you pinpoint the cause\\r\\nof the problem and continue on to resolving it."}]}')}}]);