<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Keep your ML Models Good as new Forever Blog</title>
        <link>https://WaterdipAI.github.io/website/blog</link>
        <description>Keep your ML Models Good as new Forever Blog</description>
        <lastBuildDate>Wed, 01 Feb 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[What is ML Observability?]]></title>
            <link>https://WaterdipAI.github.io/website/blog/mlobs</link>
            <guid>mlobs</guid>
            <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[ML observability is the ability to measure and report on the performance of machine learning models in real time.]]></description>
            <content:encoded><![CDATA[<p>ML observability is the ability to measure and report on the performance of machine learning models in real time.
It enables organizations to improve model accuracy and reliability by measuring service quality continuously across pre-production and production phases of model life cycles.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-could-possibly-go-wrong"><strong>What could possibly go wrong?</strong><a class="hash-link" href="#what-could-possibly-go-wrong" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-distribution"><strong>Data Distribution</strong><a class="hash-link" href="#data-distribution" title="Direct link to heading">​</a></h3><p>  In machine learning, the most basic assumption is that the distribution of data your model is exposed to changes over time.
For example, if you are working on an image recognition task and have been collecting images of cats for a few months,
and then suddenly start collecting images of dogs, it would be unwise to keep using the same model architecture. A model that has been trained on cats may not work very well on dogs
– even if there are some visual similarities between cats and dogs (big/pointy ears, tails, etc.).</p><table><thead><tr><th align="center"><img loading="lazy" alt="ML Data Distribution" src="/website/assets/images/data_distribution-ff4ef4616c960009b30e7789879313b2.png" width="777" height="397" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center"><em>Training Serving Data Distribution</em></td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-serving-skew"><strong>Training Serving Skew</strong><a class="hash-link" href="#training-serving-skew" title="Direct link to heading">​</a></h3><p>A common use case for observability is when your model is working well in training but poorly in production.
Data scientists often find that the data their model was trained on is statistically different from the data they
see in production. This discrepancy can be due to any number of factors: the sample size of your training set
may not be enough to capture all possible conditions and edge cases; some of your features may be correlated;
or there may be seasonal or event-driven variations in the data that weren't captured by your dataset.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-ensure-that-ml-model-is-working-correctly"><strong>How to ensure that ML model is working correctly?</strong><a class="hash-link" href="#how-to-ensure-that-ml-model-is-working-correctly" title="Direct link to heading">​</a></h2><table><thead><tr><th align="center"><img loading="lazy" alt="ML Monitoring Lifecycle" src="/website/assets/images/ml_monitoring_lifecycle-fb3edf35df95f1184bf7e7f13e4515d4.png" width="1440" height="766" class="img_ev3q"></th></tr></thead><tbody><tr><td align="center"><em>ML Monitoring Lifecycle</em></td></tr></tbody></table><p>ML observability has to track the lifecycle of an ML model from its inception through training, validation and deployment.
It encompasses a broad set of capabilities, including the ability to:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="test--pre-production-validation">Test &amp; Pre-Production Validation<a class="hash-link" href="#test--pre-production-validation" title="Direct link to heading">​</a></h3><p>In order to ensure that model behavior conforms to your expectations, you need to monitor the model’s performance during Pre-Production validation.
ML observability tools allow you to track a model’s performance for each defined slice in the training data, so you can see how well it will generalize when deployed into production.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="monitor-production-system">Monitor Production System<a class="hash-link" href="#monitor-production-system" title="Direct link to heading">​</a></h3><p>When model is deployed to production, ML Observability keeps track of all of the input features and output predictions to provide proactive alerts.
These alerts can be used as early warning signs of potential issues with the model.
The user can also use these alerts to debug the model by checking whether any of these inputs have changed since they last checked,
or if any of these outputs are not being predicted correctly.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="root-cause-analysis">Root Cause Analysis<a class="hash-link" href="#root-cause-analysis" title="Direct link to heading">​</a></h3><p>When a model in production is failing to perform as expected, the first step toward a resolution is to understand what happened.
This determination can be difficult because the model may have been trained and tested on different data than what it's operating on now,
or it may have been trained with different hyperparameters than those being used now.
In both cases, the network weights could have changed substantially from their training parameters, meaning that a new best-fit line wouldn't exist.</p><p>With the help of Observability platform to monitor your models in production, you'll be able to determine exactly which distributions in input data,
features, ground truth/proxy metrics have contributed to a change in the model’s performance by combining your
historical data with your model's current performance. The result of this analysis will let you pinpoint the cause
of the problem and continue on to resolving it.</p>]]></content:encoded>
            <category>ML Observability</category>
        </item>
    </channel>
</rss>